{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\n#Read data and store in dictionary by category.\nkeys = ['business','entertainment','politics','sport','tech']\ndataset = {}\nfor key in keys:\n    df = pd.read_pickle(\"../input/bbcwithlabel/train_df_label_\"+key+\".pickle\" )\n    df_result = df[['Text', 'Summary', 'text_clean', \"summary_clean\", \"labels\", \"labels_idx_list\"]].copy()\n    dataset[key] = df_result\n    \ndataset[\"business\"].head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T12:36:37.059082Z","iopub.execute_input":"2023-04-06T12:36:37.059507Z","iopub.status.idle":"2023-04-06T12:36:41.723628Z","shell.execute_reply.started":"2023-04-06T12:36:37.059368Z","shell.execute_reply":"2023-04-06T12:36:41.721343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We get dataframe have all articles without divided by category.\ndf_all_doc = pd.read_pickle(\"../input/train-df-not-shuffle/train_df_not_shuffle.pickle\" )\ndf_all_doc.tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:41.725467Z","iopub.execute_input":"2023-04-06T12:36:41.725854Z","iopub.status.idle":"2023-04-06T12:36:41.853415Z","shell.execute_reply.started":"2023-04-06T12:36:41.725817Z","shell.execute_reply":"2023-04-06T12:36:41.852523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We take all text from dataframe. Because we need to get dictionary of dataset's vocabulary.\nX_all_doc = df_all_doc[\"Text\"]\nX_all_doc = np.array(X_all_doc)\nlen(X_all_doc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\n\n#text_vectorizer is a layer that transforms list of sentences to vector of integer.\n#We padding every vector to the same length of 500 elements.\ntext_vectorizer = keras.layers.TextVectorization(max_tokens=34500, standardize=\"lower_and_strip_punctuation\",\n                                                 split=\"whitespace\", output_mode=\"int\", output_sequence_length=500)\n\n#We adapt text_vectorizer to all text so that we can have dataset's vocabulary.\ntext_vectorizer.adapt(X_all_doc, batch_size=2225)\n\n#Dataset's vocabulary is a dictionary. For example: 'hi':1, \"bye\":2.\nvocab = text_vectorizer.get_vocabulary()\nprint(\"Vocab : {}\".format(vocab[:10]))\nprint(\"Vocab Size : {}\".format(text_vectorizer.vocabulary_size()))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:41.869427Z","iopub.execute_input":"2023-04-06T12:36:41.870246Z","iopub.status.idle":"2023-04-06T12:36:50.632397Z","shell.execute_reply.started":"2023-04-06T12:36:41.870198Z","shell.execute_reply":"2023-04-06T12:36:50.631017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keys = ['business','entertainment','politics','sport','tech']\n\n#We vectorized text in dataset of every category to be a matrix of integer.\n#Each vector in matrix present a sentence in text.\n#The matrix is \"text_embedding\" attribute of dataset.\nfor key in keys:\n    df_category = dataset[key]\n    vectorized_text_list = []\n    for i in range(len(df_category)):\n        vectorized_text = text_vectorizer(df_category.iloc[i][\"text_clean\"])\n        vectorized_text = np.array(vectorized_text)\n        vectorized_text_list.append(vectorized_text)\n    df_category[\"text_embedding\"] = vectorized_text_list\n        \n#dataset[\"sport\"].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:50.711614Z","iopub.execute_input":"2023-04-06T12:36:50.711900Z","iopub.status.idle":"2023-04-06T12:36:56.581253Z","shell.execute_reply.started":"2023-04-06T12:36:50.711876Z","shell.execute_reply":"2023-04-06T12:36:56.580199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#We split dataset to train set and test set. Ratio is 9:1.\ntrain_sport, test_sport = train_test_split(dataset[\"sport\"], test_size=0.1)\ntrain_business, test_business = train_test_split(dataset[\"business\"], test_size=0.1)\ntrain_entertainment, test_entertainment = train_test_split(dataset[\"entertainment\"], test_size=0.1)\ntrain_tech, test_tech = train_test_split(dataset[\"tech\"], test_size=0.1)\ntrain_politics, test_politics = train_test_split(dataset[\"politics\"], test_size=0.1)\n\n#Then we store in dictionary by category.\ntrain_test_sets = {}\ntrain_test_sets[\"sport\"] = {\"train\": train_sport, \"test\": test_sport}\ntrain_test_sets[\"business\"] = {\"train\": train_business, \"test\": test_business}\ntrain_test_sets[\"entertainment\"] = {\"train\": train_entertainment, \"test\": test_entertainment}\ntrain_test_sets[\"tech\"] = {\"train\": train_tech, \"test\": test_tech}\ntrain_test_sets[\"politics\"] = {\"train\": train_politics, \"test\": test_politics}","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:56.592801Z","iopub.execute_input":"2023-04-06T12:36:56.593421Z","iopub.status.idle":"2023-04-06T12:36:56.667849Z","shell.execute_reply.started":"2023-04-06T12:36:56.593388Z","shell.execute_reply":"2023-04-06T12:36:56.666751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n#Save dataset for future comparing.\ntest_data_file = 'test_data.pickle'\nwith open(test_data_file, 'wb') as handle:                                     \n    pickle.dump(train_test_sets, handle)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:56.679042Z","iopub.execute_input":"2023-04-06T12:36:56.679451Z","iopub.status.idle":"2023-04-06T12:36:56.960678Z","shell.execute_reply.started":"2023-04-06T12:36:56.679428Z","shell.execute_reply":"2023-04-06T12:36:56.959887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:56.961710Z","iopub.execute_input":"2023-04-06T12:36:56.962918Z","iopub.status.idle":"2023-04-06T12:36:57.167586Z","shell.execute_reply.started":"2023-04-06T12:36:56.962858Z","shell.execute_reply":"2023-04-06T12:36:57.166517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge-score\nimport numpy as np\nimport pandas as pd\n\n\nimport re\nimport string\nimport csv\nimport os\nfrom keras.models import Sequential\nimport torch\nfrom tensorflow.keras import optimizers, utils\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Embedding,Dropout, Concatenate, TimeDistributed, Bidirectional, GRU, BatchNormalization, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom rouge_score import rouge_scorer\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:36:57.168689Z","iopub.execute_input":"2023-04-06T12:36:57.168971Z","iopub.status.idle":"2023-04-06T12:37:11.716887Z","shell.execute_reply.started":"2023-04-06T12:36:57.168946Z","shell.execute_reply":"2023-04-06T12:37:11.715713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n#Calculate F1 of ROUGE for Evaluation.\ndef calc_rouge_scores(pred_summaries, gold_summaries, \n                                 keys=['rouge1', 'rougeL'], use_stemmer=True):\n    #Calculate rouge scores\n    scorer = rouge_scorer.RougeScorer(keys, use_stemmer= use_stemmer)\n    \n    n = len(pred_summaries)\n    \n    #Calculate ROUGE score for every test in testset.\n    scores = [scorer.score(pred_summaries[j], gold_summaries[j]) for \n              j in range(n)] \n    \n    dict_scores={}                                                            \n    for key in keys:\n        dict_scores.update({key: {}})\n        \n    \n    for key in keys:\n        \n        #Get precision for every test in testset.\n        precision_list = [scores[j][key][0] for j in range(len(scores))]\n        #Get recall for every test in testset.\n        recall_list = [scores[j][key][1] for j in range(len(scores))]\n        #Get F1 for every test in testset.\n        f1_list = [scores[j][key][2] for j in range(len(scores))]\n\n        #Calculate mean ROUGE score of all test.\n        precision = np.mean(precision_list)\n        recall = np.mean(recall_list)\n        f1 = np.mean(f1_list)\n        \n        dict_results = {'recall': recall, 'precision': precision, 'f1': f1}\n        \n        dict_scores[key] = dict_results\n        \n    return dict_scores","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:11.718612Z","iopub.execute_input":"2023-04-06T12:37:11.719101Z","iopub.status.idle":"2023-04-06T12:37:11.732525Z","shell.execute_reply.started":"2023-04-06T12:37:11.718962Z","shell.execute_reply":"2023-04-06T12:37:11.731183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing every text_embedding to a matrix with shape 246,500. \n# We use padding of 0 to do this. \ndef padding_sentence(X, Y):\n    max_number_sentence = 246\n    padding_X=np.empty(500)\n    padding_X.fill(0)\n    for i in range(len(X)):\n        while(len(X[i]) < max_number_sentence):\n            X[i] = np.append(X[i], [padding_X], axis = 0)\n            Y[i] = np.append(Y[i], [0], axis = 0)\n    \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:11.734530Z","iopub.execute_input":"2023-04-06T12:37:11.734932Z","iopub.status.idle":"2023-04-06T12:37:11.748190Z","shell.execute_reply.started":"2023-04-06T12:37:11.734894Z","shell.execute_reply":"2023-04-06T12:37:11.746246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Diving batch of dataset.\ndef get_batch(tasks_key, batch_size, number_of_shot = None):\n    batch_sets = {}\n    test_sets = {}\n    for key in tasks_key:\n \n\n        #In scenario of training on 12-Shot dataset, Few-shot Learning.\n        if number_of_shot:\n            X_train = np.array(train_test_sets[key][\"train\"][\"text_embedding\"])[:number_of_shot]\n            y_train = np.array(train_test_sets[key][\"train\"][\"labels\"])[:number_of_shot]\n        \n        #In scenario of training on full dataset.\n        else:\n            X_train = np.array(train_test_sets[key][\"train\"][\"text_embedding\"])\n            y_train = np.array(train_test_sets[key][\"train\"][\"labels\"])\n\n        X_test = np.array(train_test_sets[key][\"test\"][\"text_embedding\"])\n        y_test = np.array(train_test_sets[key][\"test\"][\"labels\"])\n        \n        \n        #Standardizing text_embedding.\n        X_train, y_train = padding_sentence(X_train, y_train)\n        X_test, y_test = padding_sentence(X_test, y_test)\n        \n        \n        \n        #Calculate number of batch based on batch_size.\n        num_batches = (len(X_train) + batch_size - 1) // batch_size\n        \n        \n        #Batch_sets is a dictionary by category.\n        #Value of each category is a vector that named batches.\n        #Each element of batches is a set of data that present a batch in training.\n        \n        batches = []\n        for i in range(num_batches):\n            if batch_size*i+batch_size <= len(X_train):\n                batches.append({\"X_train\": X_train[i*batch_size:i*batch_size+batch_size],\n                                \"y_train\": y_train[i*batch_size:i*batch_size+batch_size]})\n            else:\n                batches.append({\"X_train\": X_train[i*batch_size:],\n                                \"y_train\": y_train[i*batch_size:]})\n                \n            \n        batch_sets[key]=batches\n        test_sets[key] = {\"X_test\": X_test,\n                                \"y_test\": y_test}\n        \n        \n    return batch_sets, test_sets","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:11.750498Z","iopub.execute_input":"2023-04-06T12:37:11.751203Z","iopub.status.idle":"2023-04-06T12:37:11.764221Z","shell.execute_reply.started":"2023-04-06T12:37:11.751140Z","shell.execute_reply":"2023-04-06T12:37:11.762745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, models, losses\nimport tensorflow as tf\nimport numpy as np\n\n\nclass MAML:\n    def __init__(self):\n        self.meta_model = self.get_maml_model()\n\n    def get_maml_model(self):\n        # define model\n        model = Sequential()\n        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(246, 500)))\n        model.add(Dense(1,activation='sigmoid'))\n        model.add(tf.keras.layers.Reshape((-1,), input_shape=(246,1)))\n        return model\n\n    \n    #Training step of each batch.\n    def train_on_batch(self, support_train_data, query_train_data, inner_optimizer, outer_optimizer=None):\n\n        batch_acc = []\n        batch_loss = []\n        task_weights = []\n\n        support_task_key = [\"tech\",\"politics\",\"entertainment\",\"sport\"]\n        query_task_keys = [\"business\"]\n        \n        #Get currrent model's weight to make sure that model's weight is reset in beginning\n        #of each inner loop.    \n        meta_weights = self.meta_model.get_weights()\n\n                \n        #Inner loops.\n        #Loop through all support dataset and update model weight.\n        for key in support_task_key:            \n            #Get starting initialized weight. \n            self.meta_model.set_weights(meta_weights)\n            \n            X = np.array([np.array(val) for val in support_train_data[key][\"X_train\"]])\n            y = np.array([np.array(val) for val in support_train_data[key][\"y_train\"]])\n            with tf.GradientTape() as tape:\n                pred = self.meta_model(X)\n                loss = losses.binary_crossentropy(y, pred)\n                \n            # Calculate the gradients for the variables\n            gradients = tape.gradient(loss, self.meta_model.trainable_variables)\n            # Apply the gradients and update the optimizer\n            inner_optimizer.apply_gradients(zip(gradients, self.meta_model.trainable_variables))\n           \n            #Save optimized weight of each support task. \n            task_weights.append(self.meta_model.get_weights())\n\n    \n        #Calculate loss of each optimized weight on query training dataset set.\n        with tf.GradientTape() as tape:\n            for i in range(len(support_task_key)):\n            \n                query_task_key = query_task_keys[0]\n                \n                #Get each saved optimized weight.\n                self.meta_model.set_weights(task_weights[i])\n                \n                X = np.array([np.array(val) for val in query_train_data[query_task_key][\"X_train\"]])\n                y = np.array([np.array(val) for val in query_train_data[query_task_key][\"y_train\"]])\n  \n                pred = self.meta_model(X)\n                loss = losses.binary_crossentropy(y, pred)\n                \n                batch_loss.append(loss)\n                \n            #Calculate sum loss\n            #Calculate mean loss only for visualizing.\n            sum_loss = tf.reduce_sum(batch_loss)\n            mean_loss = tf.reduce_mean(batch_loss)\n\n        #Get starting initialized weight. \n        self.meta_model.set_weights(meta_weights)\n\n        #Backpropagation of outer loop.\n        if outer_optimizer:\n            grads = tape.gradient(sum_loss, self.meta_model.trainable_variables)\n            outer_optimizer.apply_gradients(zip(grads, self.meta_model.trainable_variables))\n                      \n        return mean_loss\n        \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:11.766575Z","iopub.execute_input":"2023-04-06T12:37:11.767011Z","iopub.status.idle":"2023-04-06T12:37:11.783669Z","shell.execute_reply.started":"2023-04-06T12:37:11.766971Z","shell.execute_reply":"2023-04-06T12:37:11.782548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\n#Set parameter of training data. \n# Diving training data into batches.\n\nsupport_tasks_key = [\"sport\", \"entertainment\", \"tech\", \"politics\"]\nquery_tasks_key = [\"business\"]\n\nnumber_of_query_batch = 12\nnumber_of_shot = 12\nquery_batch_size = math.ceil(number_of_shot / number_of_query_batch)\n\n#query_batch_size = 30\nsupport_batch_size = 30\n\n\n\nsupport_batch_sets, support_test_sets = get_batch(support_tasks_key, support_batch_size)\nquery_batch_sets, query_test_sets = get_batch(query_tasks_key, query_batch_size, number_of_shot)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:11.784823Z","iopub.execute_input":"2023-04-06T12:37:11.785108Z","iopub.status.idle":"2023-04-06T12:37:23.345022Z","shell.execute_reply.started":"2023-04-06T12:37:11.785083Z","shell.execute_reply":"2023-04-06T12:37:23.343854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation while training.\ndef val_on_batch(model):\n    y_pred_list =[]\n    idx_list=[]\n\n    key_query_task=\"business\"\n\n    #Get text_embedding of test set.\n    X_test = np.array([np.array(val) for val in query_test_sets[key_query_task][\"X_test\"]])\n    \n    #Prediction on test set.\n    #Every prediction is a vector of values between 0 and 1.\n    # Each value equivalent a sentences in the same position.\n    # Each value is prob that sentence is picked for summary.\n    y_preds = model.predict(X_test, verbose=0)\n    \n    print(len(y_preds))\n    \n    #Loop through prediction.\n    # If the prob is equal or higher 0.5, we store it index.\n    # If the number of stored index if less than 5, we store 5 prob-highest index.\n    for j in range(len(y_preds)):\n        idx = []\n        for i in range(len(y_preds[j])):\n            pred_percent = y_preds[j][i]\n            if(pred_percent >= 0.5):\n                idx.append(i)\n        if len(idx) < 5.0:\n            idx = np.argsort(y_preds[j][-5:])\n        idx = sorted(idx)\n        idx_list.append(idx)\n    \n    val_sets = train_test_sets[key_query_task][\"test\"]\n\n    #Retrieve picked sentences from source texts.\n    df_text_clean = val_sets[\"text_clean\"]\n    pred_summaries = []\n    for doc in range(len(idx_list)):\n        pred_summary_sentences_list = []\n        text_clean = np.array(df_text_clean.iloc[doc])\n        idx_doc = idx_list[doc]\n        for i in range(len(text_clean)):\n            if(i in idx_doc):\n                sentence = text_clean[i]\n                pred_summary_sentences_list.append(sentence)\n                \n        pred_summary = \" \".join(pred_summary_sentences_list)\n        pred_summaries.append(pred_summary)\n    \n    \n    #Get golden summary.\n    df_gold = val_sets[\"Summary\"]\n\n    gold_summaries = [df_gold.iloc[m] for m in range(len(df_gold))]\n    \n\n\n\n    summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n\n\n    #calculate rouge score\n    scores = calc_rouge_scores(pred_summaries, gold_summaries, \n                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n    \n    return scores\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:23.346430Z","iopub.execute_input":"2023-04-06T12:37:23.346727Z","iopub.status.idle":"2023-04-06T12:37:23.358632Z","shell.execute_reply.started":"2023-04-06T12:37:23.346699Z","shell.execute_reply":"2023-04-06T12:37:23.356991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\n\n\nmaml = MAML()\n\ninner_optimizer = optimizers.Adam(learning_rate=0.001)\nouter_optimizer = optimizers.Adam(learning_rate=0.001)\n\n#print(y_test)\n\nquery_key = \"business\"\n\n#Find min number of batch.\n#Because each dataset of different category have different size.\n#So the number of batch will be different for each category.\n#We need to make sure have all the category in a training step.\ntraining_steps = 1000\nfor key in support_batch_sets:\n    if(len(support_batch_sets[key]) < training_steps):\n        training_steps = len(support_batch_sets[key])\n        \nfor key in query_batch_sets:\n    if(len(query_batch_sets[key]) < training_steps):\n        training_steps = len(query_batch_sets[key])\n\n                             \nvaluating_steps = len(query_test_sets[query_key][\"X_test\"])\n\n            \ntrain_progbar = utils.Progbar(training_steps)\n\nloss_plot = []\nf1_score_plot = []\nprecision_plot = []\nrecall_plot = []\n\n\n#Loop by number of epochs\nfor epoch in range(epochs):\n    train_meta_loss = []\n    val_meta_loss = []\n    \n    #In each epoch, we loop through each batch. Each batch will be sent to training step function.\n    for i in range(training_steps):\n        support_train_data = {}\n        query_train_data = {}\n        \n        for support_key in support_batch_sets:\n            support_train_data[support_key] = support_batch_sets[support_key][i]\n        for query_key in query_batch_sets: \n            query_train_data[query_key] = query_batch_sets[query_key][i]\n            \n        batch_train_loss = maml.train_on_batch(support_train_data,\n                                                        query_train_data,\n                                                        inner_optimizer,\n                                                        outer_optimizer=outer_optimizer)\n\n        train_meta_loss.append(batch_train_loss)\n        train_progbar.update(i+1, [('loss', np.mean(train_meta_loss))])\n    \n    #Store number for ploting\n    loss_plot.append( np.mean(train_meta_loss))\n    \n    scores = val_on_batch(maml.meta_model)\n    f1_score_plot.append(scores[\"rouge1\"][\"f1\"])\n    precision_plot.append(scores[\"rouge1\"][\"precision\"])\n    recall_plot.append(scores[\"rouge1\"][\"recall\"])\n    \n    print(\"\\n\")\n    print(scores)\n    print(\"\\n\")\n    \n    \n\n\n#Save trained model\nmaml.meta_model.save(\"./model.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:37:23.360527Z","iopub.execute_input":"2023-04-06T12:37:23.360963Z","iopub.status.idle":"2023-04-06T12:41:44.962426Z","shell.execute_reply.started":"2023-04-06T12:37:23.360923Z","shell.execute_reply":"2023-04-06T12:41:44.961308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom matplotlib import pyplot as plt\n\n#Ploting F1 and loss graph.\nplt.title('model accuracy') \nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n\nepochs_plot = [i for i in range(epochs)]\n\n\nplt.plot(epochs_plot, loss_plot , color=\"red\", label = \"loss\")\nplt.plot(epochs_plot, f1_score_plot, color=\"blue\", label = \"validation\")\n\nplt.legend(loc='upper left')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:41:44.963913Z","iopub.execute_input":"2023-04-06T12:41:44.964286Z","iopub.status.idle":"2023-04-06T12:41:45.164149Z","shell.execute_reply.started":"2023-04-06T12:41:44.964252Z","shell.execute_reply":"2023-04-06T12:41:45.163232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom matplotlib import pyplot as plt\n\n#Ploting Recall and Precision graph.\nplt.title('rouge score')\nplt.ylabel('score')\nplt.xlabel('epoch')\n\nepochs_plot = [i for i in range(epochs)]\n\nplt.plot(epochs_plot, recall_plot, color=\"red\", label = \"recall\")\nplt.plot(epochs_plot, precision_plot, color=\"blue\", label = \"precision\")\n\nplt.legend(loc='upper left')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:41:45.165650Z","iopub.execute_input":"2023-04-06T12:41:45.166359Z","iopub.status.idle":"2023-04-06T12:41:45.331817Z","shell.execute_reply.started":"2023-04-06T12:41:45.166316Z","shell.execute_reply":"2023-04-06T12:41:45.330346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This cell is the same to cell \"val_on_batch\".\n#It calculate score,save pair golden_summary-model_summary, save evaluation information.\noutput_file = 'result.pickle'\n\n\ny_pred_list =[]\nidx_list=[]\n\nkey_query_task=\"business\"\n\nX_test = np.array([np.array(val) for val in query_test_sets[key_query_task][\"X_test\"]])\n \n    \ny_preds = maml.meta_model.predict(X_test, verbose=0)\n    \n\n    \nfor j in range(len(y_preds)):\n    idx = []\n    for i in range(len(y_preds[j])):\n        pred_percent = y_preds[j][i]\n        if(pred_percent > 0.5):\n            idx.append(i)\n    if len(idx) < 5.0:\n        idx = np.argsort(y_preds[j][-5:])\n    idx = sorted(idx)\n    idx_list.append(idx)\n    \nval_sets = train_test_sets[key_query_task][\"test\"]\n\n#retrieve summary pairs\ndf_text_clean = val_sets[\"text_clean\"]\npred_summaries = []\nfor doc in range(len(idx_list)):\n    pred_summary_sentences_list = []\n    text_clean = np.array(df_text_clean.iloc[doc])\n    idx_doc = idx_list[doc]\n    for i in range(len(text_clean)):\n        if(i in idx_doc):\n            sentence = text_clean[i]\n            pred_summary_sentences_list.append(sentence)\n                \n    pred_summary = \" \".join(pred_summary_sentences_list)\n    pred_summaries.append(pred_summary)\n    \n    \ndf_gold = val_sets[\"Summary\"]\n\ngold_summaries = [df_gold.iloc[m] for m in range(len(df_gold))]\n    \n\n\n\nsummaries_comp = tuple(zip(pred_summaries, gold_summaries))\n\n\n#calculate rouge score\nscores = calc_rouge_scores(pred_summaries, gold_summaries, \n                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n    \nresults_dict ={'summaries_comp': summaries_comp,\n               'sent_index_number': idx, 'Rouge': scores, 'mod_summary': maml.meta_model.summary()}\n\nwith open(output_file, 'wb') as handle:                                     \n    pickle.dump(results_dict, handle)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:41:45.333293Z","iopub.execute_input":"2023-04-06T12:41:45.333674Z","iopub.status.idle":"2023-04-06T12:41:46.369151Z","shell.execute_reply.started":"2023-04-06T12:41:45.333638Z","shell.execute_reply":"2023-04-06T12:41:46.368211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.read_pickle(\"./result.pickle\")\nprint(result[\"Rouge\"])\nprint(\"\\nPrediction\\n\")\nprint(result[\"summaries_comp\"][0][0])\nprint(\"\\nReal summary\\n\")\nprint(result[\"summaries_comp\"][0][1])","metadata":{"execution":{"iopub.status.busy":"2023-04-06T12:41:46.370512Z","iopub.execute_input":"2023-04-06T12:41:46.370783Z","iopub.status.idle":"2023-04-06T12:41:46.377800Z","shell.execute_reply.started":"2023-04-06T12:41:46.370758Z","shell.execute_reply":"2023-04-06T12:41:46.376080Z"},"trusted":true},"execution_count":null,"outputs":[]}]}