{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport spacy\nfrom sentence_transformers import SentenceTransformer\nfrom datetime import datetime as dt\n\n\n### Function help to separate text to list of sentences\ndef text_to_sent_list(text, \n                      nlp = spacy.load(\"en_core_web_lg\"), \n                      embedder = SentenceTransformer('distilbert-base-nli-mean-tokens'),\n                      min_len=2):\n    \n    #convert to list of sentences\n    text = nlp(text)\n    sents = list(text.sents)\n    #remove newline\n    sents_clean = [sentence.replace('\\n', ' ') for sentence in sents]\n    #remove entries with empty list\n    sents_clean = [sentence.text for sentence in sents_clean if len(sentence)!=0]\n    #remove entries with only white space\n    sents_clean = [sentence.text for sentence in sents_clean if sentence != \" \"]\n    #embed sentences. We only use this step for adding \"label\" attribute.\n    sents_embedding= np.array(embedder.encode(sents_clean, convert_to_tensor=True))\n    \n    return sents_clean, sents_embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read data\ndata_business = pd.read_csv('../input/bbcnewstocsv/businessDataset.csv')\ndata_entertainment = pd.read_csv('../input/bbcnewstocsv/entertainmentDataset.csv')\ndata_politics = pd.read_csv('../input/bbcnewstocsv/politicsDataset.csv')\ndata_tech = pd.read_csv('../input/bbcnewstocsv/techDataset.csv')\ndata_sport = pd.read_csv('../input/bbcnewstocsv/sportDataset.csv', encoding= 'unicode_escape')\n\n#Drop unused column\ndata_business = data_business.drop(['ID'], axis=1)\ndata_business = data_business.reset_index(drop=True)\ndata_entertainment = data_entertainment.drop(['ID'], axis=1)\ndata_entertainment = data_entertainment.reset_index(drop=True)\ndata_politics = data_politics.drop(['ID'], axis=1)\ndata_politics = data_politics.reset_index(drop=True)\ndata_tech = data_tech.drop(['ID'], axis=1)\ndata_tech = data_tech.reset_index(drop=True)\ndata_sport = data_sport.drop(['ID'], axis=1)\ndata_sport = data_sport.reset_index(drop=True)\n\n# make dictionary of datasets by category\ndatasets = {\"business\": data_business, \n        \"entertainment\": data_entertainment,\n        \"politics\":data_politics,\n        \"sport\":data_sport,\n        \"tech\":data_tech}\n\ndatasets[\"business\"].head()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T07:49:56.386646Z","iopub.status.idle":"2023-02-16T07:49:56.387268Z","shell.execute_reply.started":"2023-02-16T07:49:56.386940Z","shell.execute_reply":"2023-02-16T07:49:56.386972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in datasets:\n    \n    dataset = datasets[key]\n    output_file = 'train_df_'+key+'.pickle' \n    \n    #load nlp and embedder\n    nlp = spacy.load(\"en_core_web_lg\")\n    embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n\n    #extract clean sentence list and sentence embedding for each article's text\n    f = lambda text: text_to_sent_list(text, nlp=nlp, embedder=embedder, min_len=2)\n    s_interim_tuple = dataset['Text'].apply(f)\n\n    dataset['text_clean'] = s_interim_tuple.apply(lambda x: x[0])\n    dataset['text_embedding'] = s_interim_tuple.apply(lambda x: x[1])\n\n    #extract clean sentence list and sentence embedding for each article's summary\n    f = lambda summ: text_to_sent_list(summ, nlp=nlp, embedder=embedder, min_len=0)\n    s_interim_tuple = dataset['Summary'].apply(f)\n\n    dataset['summary_clean'] = s_interim_tuple.apply(lambda x: x[0])\n    dataset['summary_embedding'] = s_interim_tuple.apply(lambda x: x[1])\n\n    with open(output_file, 'wb') as handle:                                     \n        pickle.dump(dataset, handle)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T07:49:56.394385Z","iopub.status.idle":"2023-02-16T07:49:56.394845Z","shell.execute_reply.started":"2023-02-16T07:49:56.394643Z","shell.execute_reply":"2023-02-16T07:49:56.394664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom datetime import datetime as dt\n\n### Helper Functions\n\n\n#Calculate cosine between two vector for checking similarity.\ndef find_sim_single_summary(summary_sentence_embed, doc_emedding):\n    cos_sim_mat = cosine_similarity(doc_emedding, summary_sentence_embed)\n    #Pick most similar sentences.\n    idx_arr = np.argmax(cos_sim_mat, axis=0)\n    return idx_arr\n\n#Adding attribute \"label\" for data.\ndef label_sent_in_summary(s_text, s_summary):\n    doc_num = s_text.shape[0]\n    \n    #initialize zeros. All sentences is labeled by \"0\"\n    labels = [np.zeros(doc.shape[0]) for doc in s_text.tolist()] \n    \n    #find index of summary-picked sentences. Check every pair sentences of text and summary.\n    idx_list = [np.sort(find_sim_single_summary(s_summary[j], s_text[j])) for j \n                                                            in range(doc_num)]\n    #Change label to \"1\" for summary-picked sentences  \n    for j in range(doc_num):\n        labels[j][idx_list[j]]= 1 \n    \n    return idx_list, labels\n","metadata":{"execution":{"iopub.status.busy":"2023-02-16T07:49:56.397001Z","iopub.status.idle":"2023-02-16T07:49:56.397547Z","shell.execute_reply.started":"2023-02-16T07:49:56.397291Z","shell.execute_reply":"2023-02-16T07:49:56.397316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_pickle('../input/bbcpreprocess/train_df_tech.pickle' )\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-16T07:49:56.399542Z","iopub.status.idle":"2023-02-16T07:49:56.400320Z","shell.execute_reply.started":"2023-02-16T07:49:56.399826Z","shell.execute_reply":"2023-02-16T07:49:56.399861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndatasets_key=[\"sport\",\"politics\",\"business\",\"tech\",\"entertainment\"]\nfor key in datasets_key:\n    output_file = 'train_df_label_'+key+'.pickle'\n\n    df = pd.read_pickle('../input/bbcpreprocess/train_df_'+key+'.pickle' )\n\n    #get index list and target labels\n    idx_list, labels = label_sent_in_summary(df.text_embedding, df.summary_embedding)\n\n    #wrap in dataframe\n    df['labels'] = labels\n    df['labels_idx_list'] = idx_list\n\n    #save to pickle\n    with open(output_file, 'wb') as handle:                                     \n        pickle.dump(df, handle)","metadata":{"execution":{"iopub.status.busy":"2023-02-16T07:49:56.403258Z","iopub.status.idle":"2023-02-16T07:49:56.403711Z","shell.execute_reply.started":"2023-02-16T07:49:56.403506Z","shell.execute_reply":"2023-02-16T07:49:56.403528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}